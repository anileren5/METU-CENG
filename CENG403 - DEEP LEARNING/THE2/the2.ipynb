{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIi6vhnurJA-"
      },
      "source": [
        "# THE2 - CENG403 Spring 2024\n",
        "\n",
        "This document contains the backbone structure for the take-home exam. You should complete this template for your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 0 Some helper functions for algebraic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def vector_vector_product(v1, v2):\n",
        "    if (len(v1) != len(v2)):\n",
        "        raise ValueError(\"Dimensions do not match for vector - vector product!\")\n",
        "    result = sum([x * y for x,y in zip(v1, v2)]) \n",
        "    return result\n",
        "\n",
        "def vector_matrix_product(v, m):       \n",
        "    if (len(v) != len([row[0] for row in m])):\n",
        "        raise ValueError(\"Dimensions do not match for vector - matrix product!\")\n",
        "    result = []\n",
        "    l = len(m[0])\n",
        "    for i in range(l):\n",
        "        result.append(vector_vector_product(v, [row[i] for row in m]))\n",
        "    return result\n",
        "\n",
        "def vector_cube_product(v,c): # Here cube means thread dimensional tensor.\n",
        "    if (len(v) != len([row[0] for row in c[0]])):\n",
        "        raise ValueError(\"Dimensions do not match for vector - cube product!\")\n",
        "    result = []\n",
        "    l = len(c)\n",
        "    for i in range(l):\n",
        "        result.append(vector_matrix_product(v, c[i]))\n",
        "    return result\n",
        "\n",
        "def create_identity_matrix(n):\n",
        "    return [[1 if i == j else 0 for j in range(n)] for i in range(n)]\n",
        "\n",
        "def create_zero_square_matrix(n):\n",
        "    result = []\n",
        "    for i in range(n):\n",
        "        temp = []\n",
        "        for j in range(n):\n",
        "            temp.append(n)\n",
        "        result.append(temp)\n",
        "    return result\n",
        "\n",
        "def create_diagonal_matrix(diagonal_entry, n):\n",
        "    return [[diagonal_entry if i == j else 0 for j in range(n)] for i in range(n)]\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kI43OQKUTmrV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def cekirdek(sayi: int):\n",
        "    #Sets the seed for random number generation\n",
        "    random.seed(sayi)\n",
        "\n",
        "def rastgele_dogal(boyut, aralik=None, dagilim='uniform'):\n",
        "    \"\"\"\n",
        "    Generates data of specified dimensions with random integer values and returns a gergen object.\n",
        "\n",
        "    Parameters:\n",
        "    boyut (tuple): Shape of the desired data.\n",
        "    aralik (tuple, optional): (min, max) specifying the range of random values. Defaults to None, which implies a default range.\n",
        "    dagilim (string, optional): Distribution of random values ('uniform' or other types). Defaults to 'uniform'.\n",
        "\n",
        "    Returns:\n",
        "    gergen: A new gergen object with random integer values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set a default range if aralik is not provided\n",
        "    if aralik is None:\n",
        "        aralik = (0, 10)\n",
        "\n",
        "    def generate_random_data(shape):\n",
        "        if len(shape) == 1:\n",
        "            return [random_value(aralik, dagilim) for _ in range(shape[0])]\n",
        "        else:\n",
        "            return [generate_random_data(shape[1:]) for _ in range(shape[0])]\n",
        "\n",
        "    def random_value(aralik, dagilim):\n",
        "        if dagilim == 'uniform':\n",
        "            return random.randint(*aralik)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported distribution: {dagilim}\")\n",
        "\n",
        "    data = generate_random_data(boyut)\n",
        "    return gergen(data)\n",
        "\n",
        "def rastgele_gercek(boyut, aralik=(0.0, 1.0), dagilim='uniform', mean=0, standard_deviation=1):\n",
        "    \"\"\"\n",
        "    Generates a gergen of specified dimensions with random floating-point values.\n",
        "\n",
        "    Parameters:\n",
        "    boyut (tuple): Shape of the desired gergen.\n",
        "    aralik (tuple, optional): (min, max) specifying the range of random values. Defaults to (0.0, 1.0) for uniform distribution.\n",
        "    dagilim (string, optional): Distribution of random value (e.g., 'uniform', 'gaussian'). Defaults to 'uniform'.\n",
        "\n",
        "    Returns:\n",
        "    gergen: A new gergen object with random floating-point values.\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_random_data(shape):\n",
        "        if len(shape) == 1:\n",
        "            return [random_value(aralik, dagilim) for _ in range(shape[0])]\n",
        "        else:\n",
        "            return [generate_random_data(shape[1:]) for _ in range(shape[0])]\n",
        "\n",
        "    def random_value(aralik, dagilim, mean=mean, standard_deviation=standard_deviation):\n",
        "        if dagilim == 'uniform':\n",
        "            return random.uniform(*aralik)\n",
        "        elif dagilim == 'gaussian':\n",
        "            mean, std_dev = aralik\n",
        "            return random.gauss(mean, std_dev)\n",
        "        elif dagilim == \"normal\":\n",
        "            return random.normalvariate(mean, standard_deviation)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported distribution: {dagilim}\")\n",
        "\n",
        "    data = generate_random_data(boyut)\n",
        "    return gergen(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nsyaG2xnv1or"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from typing import Union\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoqJr35ILDwi"
      },
      "source": [
        "## 1.2 Operation Class Definition\n",
        "\n",
        "You can find the latest version of the Operation class here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZmQuuHjnwjWa"
      },
      "outputs": [],
      "source": [
        "class Operation:\n",
        "    def __call__(self, *operands, **kwargs):\n",
        "        \"\"\"\n",
        "        Modified to accept keyword arguments as well.\n",
        "        \"\"\"\n",
        "        self.operands = operands\n",
        "        self.kwargs = kwargs  # Store keyword arguments separately\n",
        "        self.outputs = None\n",
        "        return self.ileri(*operands, **kwargs)\n",
        "\n",
        "    def ileri(self, *operands, **kwargs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the operation.\n",
        "        Must be implemented by subclasses to perform the actual operation.\n",
        "\n",
        "        Parameters:\n",
        "            *operands: Variable length operand list.\n",
        "            **kwargs: Variable length keyword argument list.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: If not overridden in a subclass.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Defines the backward pass of the operation.\n",
        "        Must be implemented by subclasses to compute the gradients.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input: The gradient of the loss w.r.t. the output of this operation.\n",
        "\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcmII57SLU4_"
      },
      "source": [
        "## 1.3 Implemented Operations\n",
        "The section contains all implementations from THE1. You can customize any part as you like, and you need to complete the `TODO` sections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpWNwPiIMjTw"
      },
      "source": [
        "### 1.3.1 Add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ve5bAhtp7RbR"
      },
      "outputs": [],
      "source": [
        "class Add(Operation):\n",
        "    def ileri(self, a, b):\n",
        "        \"\"\"\n",
        "        Adds two gergen objects or a gergen object and a scalar.\n",
        "        You can modify this function.\n",
        "        \"\"\"\n",
        "        if isinstance(a, gergen) and isinstance(b, gergen):\n",
        "            self.operands = [a, b]\n",
        "            result = gergen(self.add_gergen(a.duzlestir().listeye(), b.duzlestir().listeye()), operation=self)\n",
        "            result.boyutlandir(a.boyut())\n",
        "        elif isinstance(a, gergen) and isinstance(b, (list)):\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.add_list(a.listeye(), b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (list)):\n",
        "            self.operands = [b]\n",
        "            result = gergen(self.add_list(b.listeye(), a), operation=self)\n",
        "        elif isinstance(a, gergen) and isinstance(b, (int, float)):\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.add_scalar(a.listeye(), b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (int, float)):\n",
        "            self.operands = [b]\n",
        "            result = gergen(self.add_scalar(b.listeye(), a), operation=self)\n",
        "        else:\n",
        "            raise ValueError(\"Add operation requires at least one gergen operand.\")\n",
        "\n",
        "        result.requires_grad = True\n",
        "        return result\n",
        "\n",
        "    def add_scalar(self, a, scalar):\n",
        "        if isinstance(a, list):\n",
        "            return [self.add_scalar(elem, scalar) for elem in a]\n",
        "        else:\n",
        "            return a + scalar\n",
        "\n",
        "    def add_gergen(self, a, b):\n",
        "        # Check if 'a' is a list\n",
        "        if isinstance(a, list):\n",
        "            # Check if 'b' is a list\n",
        "            if isinstance(b, list):\n",
        "                if len(a) != len(b):\n",
        "                    raise ValueError(\"Dimensions of gergen objects do not match for addition.\")\n",
        "                return [a[i] + b[i] for i in range(len(a))]\n",
        "            # If 'a' is a list and 'b' is a scalar\n",
        "            elif not isinstance(b, list):\n",
        "                return [item + b for item in a]\n",
        "\n",
        "        # If 'a' is a scalar and 'b' is a list\n",
        "        elif not isinstance(a, list) and isinstance(b, list):\n",
        "            return [a + item for item in b]\n",
        "        # Direct addition for scalars, or fallback error for unsupported types\n",
        "        elif not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a + b\n",
        "\n",
        "    def add_list(self, a, b):\n",
        "        # Check if 'a' is a list\n",
        "        if isinstance(a, list) and isinstance(b, list):\n",
        "            return [self.add_list(elem_a, elem_b) for elem_a, elem_b in zip(a, b)]\n",
        "        # If 'a' is list and b is scalar\n",
        "        elif isinstance(a, list) and not isinstance(b, list):\n",
        "            return [self.add_list(elem_a, b) for elem_a in a]\n",
        "        elif not isinstance(a, list) and isinstance(b, list):\n",
        "            return [self.add_list(a, elem_b) for elem_b in b]\n",
        "        elif not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a + b\n",
        "\n",
        "    def vector_dot_product(v1, v2):\n",
        "        if len(v1) != len(v2):\n",
        "            raise ValueError(\"Vectors must have the same length for dot product.\")\n",
        "        return sum(x * y for x, y in zip(v1, v2))\n",
        "\n",
        "    def matrix_multiply(m1, m2):\n",
        "        if len(m1[0]) != len(m2):\n",
        "            raise ValueError(\"The number of columns in the first matrix must match the number of rows in the second matrix.\")\n",
        "        return [[sum(a * b for a, b in zip(row_a, col_b)) for col_b in zip(*m2)] for row_a in m1]\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        '''\n",
        "        TODO: Implement the gradient computation for the Add operation.\n",
        "        Note: I will only handle the case in which a and b are both gergens an vectors.\n",
        "        '''\n",
        "        \n",
        "        a = self.operands[0].get_veri()\n",
        "        b = self.operands[1].get_veri()\n",
        "        n = len(a)\n",
        "        gradient = create_identity_matrix(n) # This is the current gradient that is gradient of the result of the operation with respect to operands.\n",
        "        cumulative_gradient = vector_matrix_product(grad_input.get_veri(), gradient)\n",
        "        \n",
        "        # Gradient with respect to a and b will be the same. So, I will return only one value. It can be used for both operand.\n",
        "        return gergen(cumulative_gradient, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tgY1stEKtflW"
      },
      "outputs": [],
      "source": [
        "def apply_elementwise(g, func):\n",
        "    \"\"\"\n",
        "    Applies a given function element-wise to the data in a gergen object.\n",
        "    This version is capable of handling nested lists of any depth.\n",
        "    \"\"\"\n",
        "\n",
        "    def recursive_apply(data):\n",
        "        if isinstance(data, list):\n",
        "            # Recursively apply func to each element if data is a list\n",
        "            return [recursive_apply(sublist) for sublist in data]\n",
        "        else:\n",
        "            # Apply func directly if data is a scalar (non-list)\n",
        "            return func(data)\n",
        "\n",
        "    # Use the recursive function to apply the operation to the gergen object's data\n",
        "    return recursive_apply(g.listeye())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Mul(Operation): \n",
        "    def ileri(self, a, W):\n",
        "        \"\"\"\n",
        "        Computes ab where a is a vector and b is a matrix. This will be used to calculate nets in the Katman Class.\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(a, gergen) or not isinstance(W, gergen):\n",
        "            raise ValueError(\"Both operands must be gergen objects.\")\n",
        "\n",
        "        if not (type(a.get_veri()) == list and type(a.get_veri()[0]) != list):\n",
        "            raise ValueError(\"The first operator must be a vector gergen!\")\n",
        "        \n",
        "        if (type(a.get_veri()) == list and type(a.get_veri()[0]) == list and type(a.get_veri()[0][0]) != list):\n",
        "            raise ValueError(\"The second operator must be a matrix gergen!\")\n",
        "\n",
        "        d1 = len(a.get_veri())\n",
        "        temp = []\n",
        "        for row in W.get_veri():\n",
        "            temp.append(row[0])\n",
        "        d2 = len(temp)\n",
        "\n",
        "        if d1 != d2:\n",
        "            raise ValueError(\"Dimensions do not match!\")\n",
        "        \n",
        "        m = len(W.get_veri())\n",
        "        n = len(W.get_veri()[0])\n",
        "\n",
        "        result = []\n",
        "        \n",
        "        for i in range(n):\n",
        "            sum = 0\n",
        "            for j in range(m):\n",
        "                sum += (a.get_veri()[j] * W.get_veri()[j][i])\n",
        "            result.append(sum)\n",
        "\n",
        "        self.operands = [a, W]\n",
        "        result = gergen(result, operation=self) \n",
        "        result.requires_grad = True\n",
        "        result.requires_update = False\n",
        "        self.outputs = result\n",
        "        return result\n",
        "\n",
        "    def geri(self, grad_input, direction = \"up\"):\n",
        "        \"\"\"\n",
        "        TODO(Optional): Implement the gradient computation for the Mul operation.\n",
        "        \"\"\"\n",
        "        if direction == \"up\":\n",
        "            gradient = []\n",
        "            a = self.operands[0].get_veri()\n",
        "            W = self.operands[1].get_veri()\n",
        "            n = len(a)\n",
        "            m = len(W[0])\n",
        "            for i in range(n):\n",
        "                gradient.append(create_diagonal_matrix(a[i], m))\n",
        "\n",
        "            cumulative_gradient = vector_cube_product(grad_input.get_veri(), gradient)\n",
        "\n",
        "            return gergen(cumulative_gradient, requires_grad=False)\n",
        "        else:\n",
        "            W = self.operands[1]\n",
        "            gradient = W.devrik().get_veri()\n",
        "            cumulative_gradient = vector_matrix_product(grad_input.get_veri(), gradient)\n",
        "            if self.operands[0].operation == None:\n",
        "                return gergen(cumulative_gradient, requires_grad=False)\n",
        "            else:\n",
        "                return gergen(cumulative_gradient, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFDvrE8pPyTT"
      },
      "source": [
        "### 1.3.13 Gergen Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "oJ76D4AIO2q9"
      },
      "outputs": [],
      "source": [
        "class gergen:\n",
        "\n",
        "    #TODO: You should modify this class implementation\n",
        "\n",
        "    __veri = None  # A nested list of numbers representing the data\n",
        "    D = None  # Transpose of data\n",
        "    turev = None  # Stores the derivate (gradient of loss with respect to this gergen)\n",
        "    operation = None  # Stores the operation that produced the gergen\n",
        "    __boyut = None  # Dimensions of the gergen (Shape)\n",
        "    requires_grad = True  # Flag to determine if the gradient should be computed\n",
        "\n",
        "    def __init__(self, veri=None, operation=None, requires_grad=None, requires_update=None):\n",
        "        # The constructor for the 'gergen' class.\n",
        "        if veri is None:\n",
        "            self.__veri = []\n",
        "            self.__boyut = (0,)\n",
        "            self.D = None\n",
        "        else:\n",
        "            self.__veri = veri\n",
        "            self.__boyut = self.get_shape(veri, ())  # Assuming rectangular data\n",
        "            self.D = None\n",
        "        self.requires_grad = requires_grad\n",
        "        self.operation = operation\n",
        "        self.requires_update = requires_update\n",
        "\n",
        "    def __iter__(self):\n",
        "        # The __iter__ method returns the iterator object itself.\n",
        "        # You can reset the iterator here if you want to allow multiple passes over the data.\n",
        "        pass\n",
        "\n",
        "    def __next__(self):\n",
        "        # The __next__ method should return the next value from the iterator.\n",
        "        pass\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Allows for indexing or slicing the gergen object's data.\n",
        "\n",
        "        Parameters:\n",
        "        key (int, slice, tuple): An integer or slice for one-dimensional indexing,\n",
        "                                    or a tuple for multi-dimensional indexing/slicing.\n",
        "\n",
        "        Returns:\n",
        "        The element or a new gergen object corresponding to the provided key.\n",
        "        \"\"\"\n",
        "\n",
        "        # Helper function to handle recursive indexing/slicing\n",
        "        def index_or_slice(data, key):\n",
        "            if isinstance(key, int) or isinstance(key, slice):\n",
        "                return data[key]\n",
        "            elif isinstance(key, tuple):\n",
        "                result = data\n",
        "                for k in key:\n",
        "                    result = index_or_slice(result, k)\n",
        "                return result\n",
        "            else:\n",
        "                raise TypeError(f\"Invalid index type: {type(key)}\")\n",
        "\n",
        "        # Perform the indexing or slicing operation\n",
        "        result = index_or_slice(self.__veri, key)\n",
        "\n",
        "        # If the result is a list, return it wrapped in a new gergen object\n",
        "        return gergen(result)\n",
        "\n",
        "    def __str__(self):\n",
        "        # Generates a string representation\n",
        "        if self.uzunluk() == 0:\n",
        "            return \"Empty Gergen\"\n",
        "        else:\n",
        "            shape_str = \"\"\n",
        "            for b in self.boyut():\n",
        "                shape_str += str(b) + \"x\"\n",
        "            if shape_str == \"\":\n",
        "                shape_str += \"0x\"\n",
        "            return shape_str[:-1] + \" boyutlu gergen:\" + \"\\n\" + self.str_helper(self.listeye(), len(self.boyut()))\n",
        "\n",
        "    def str_helper(self, data, shape, depth=0):\n",
        "        if not shape:\n",
        "            return str(data)\n",
        "        elif not isinstance(data[0], list):\n",
        "            return str(data)\n",
        "        else:\n",
        "            inner_results = []\n",
        "            for subdata in data:\n",
        "                inner_results.append(self.str_helper(subdata, shape, depth + 1))\n",
        "\n",
        "            result = \"[\" + (\"\\n\" * (shape - depth - 1)).join(r for r in inner_results) + \"]\"\n",
        "            return result\n",
        "\n",
        "    @staticmethod\n",
        "    def get_shape(lst, shape=()):\n",
        "        if not isinstance(lst, list):\n",
        "            # base case\n",
        "            return shape\n",
        "        # peek ahead and assure all lists in the next depth\n",
        "        # have the same length\n",
        "        if isinstance(lst[0], list):\n",
        "            l = len(lst[0])\n",
        "            if not all(len(item) == l for item in lst):\n",
        "                msg = 'not all lists have the same length'\n",
        "                raise ValueError(msg)\n",
        "\n",
        "        shape += (len(lst),)\n",
        "        # recurse\n",
        "        shape = gergen.get_shape(lst[0], shape)\n",
        "\n",
        "        return shape\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_zeros(shape):\n",
        "        \"\"\"\n",
        "        Creates a multi-dimensional array of zeros with the specified shape.\n",
        "\n",
        "        Parameters:\n",
        "        shape (tuple): A tuple representing the dimensions of the array.\n",
        "\n",
        "        Returns:\n",
        "        A nested list (multi-dimensional array) filled with zeros.\n",
        "        \"\"\"\n",
        "        if not shape:  # If shape is empty or reaches the end of recursion\n",
        "            return 0\n",
        "        # Recursively build nested lists\n",
        "        return [gergen.custom_zeros(shape[1:]) for _ in range(shape[0])]\n",
        "\n",
        "    # HELPER\n",
        "    @staticmethod\n",
        "    def prod(iterable):\n",
        "        \"\"\"Utility function to calculate the product of elements in an iterable.\"\"\"\n",
        "        result = 1\n",
        "        for i in iterable:\n",
        "            result *= i\n",
        "        return result\n",
        "\n",
        "    def __add__(self, other):\n",
        "        add_operation = Add()\n",
        "        result_gergen = add_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        add_operation = Add()\n",
        "        result_gergen = add_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def get_veri(self):\n",
        "        return self.__veri\n",
        "\n",
        "    def uzunluk(self):\n",
        "        # Returns the total number of elements in the gergen\n",
        "        total = 1\n",
        "        for ele in self.__boyut:\n",
        "            total *= ele\n",
        "        return total\n",
        "\n",
        "    def boyut(self):\n",
        "        # Returns the shape of the gergen\n",
        "        return self.__boyut\n",
        "\n",
        "    def devrik(self):\n",
        "        # Returns the transpose of gergen\n",
        "        # Check if the gergen object is scalar\n",
        "        if self.uzunluk() == 1:\n",
        "            return gergen(self.__veri)\n",
        "        # Check if the gergen object represents a 1D list (vector)\n",
        "        if isinstance(self.__veri, list) and all(not isinstance(item, list) for item in self.__veri):\n",
        "            # Convert each element into a list (column vector)\n",
        "            return gergen([[item] for item in self.__veri])\n",
        "        else:\n",
        "            # Handle higher-dimensional cases (e.g., 2D matrices, 3D tensors, etc.)\n",
        "            new_boyut = tuple(reversed(self.__boyut))\n",
        "            order = list(reversed(range(len(self.__boyut))))\n",
        "            arr = self.custom_zeros(new_boyut)  # Assuming custom_zeros initializes an array with the given shape\n",
        "            paths = [0] * len(self.__boyut)\n",
        "            while paths[0] < self.__boyut[0]:\n",
        "                ref = self.listeye()\n",
        "                place = arr\n",
        "                for i in range(len(paths) - 1):\n",
        "                    ref = ref[paths[i]]\n",
        "                    place = place[paths[order[i]]]\n",
        "\n",
        "                place[paths[order[-1]]] = ref[paths[-1]]\n",
        "                paths[-1] += 1\n",
        "                for i in range(len(paths) - 1, 0, -1):\n",
        "                    if paths[i] >= self.__boyut[i]:\n",
        "                        paths[i] = 0\n",
        "                        paths[i - 1] += 1\n",
        "                    else:\n",
        "                        break\n",
        "            self.D = gergen(arr)\n",
        "            return gergen(arr)\n",
        "\n",
        "    def listeye(self):\n",
        "        # Converts the gergen object into a list or a nested list, depending on its dimensions.\n",
        "        if isinstance(self.__veri, list):\n",
        "            if not self.__veri:\n",
        "                return []\n",
        "            return self.__veri.copy()\n",
        "        else:\n",
        "            return self.__veri\n",
        "\n",
        "    def get_boyut(self):\n",
        "        return self.__boyut\n",
        "\n",
        "    def duzlestir(self):\n",
        "        \"\"\"Flattens a multidimensional list (self.__veri) into a 1D list.\"\"\"\n",
        "        if not isinstance(self.__veri, list):\n",
        "            return gergen(self.__veri)\n",
        "        flattened_list = []\n",
        "        # Create a stack with the initial list\n",
        "        stack = [self.__veri]\n",
        "\n",
        "        # Process the stack\n",
        "        while stack:\n",
        "            current_item = stack.pop()\n",
        "            if isinstance(current_item, list):\n",
        "                # Extend the stack by reversing the current item list\n",
        "                # to maintain the original order in the flattened list\n",
        "                stack.extend(current_item[::-1])\n",
        "            else:\n",
        "                # If it's not a list, add it to the flattened list\n",
        "                flattened_list.append(current_item)\n",
        "\n",
        "        # Since we're appending elements to the end, but processing the stack in LIFO order,\n",
        "        # we need to reverse the flattened list to restore the original element order\n",
        "        flattened_list.reverse()\n",
        "\n",
        "        # Create a new gergen instance with the flattened list\n",
        "        return gergen(flattened_list)\n",
        "\n",
        "    def boyutlandir(self, yeni_boyut):\n",
        "        \"\"\"Reshapes the gergen object to a new shape 'yeni_boyut', specified as a tuple.\"\"\"\n",
        "        # Flatten the data first\n",
        "        flat_data = list(self.duzlestir().__veri)\n",
        "\n",
        "        def reshape_helper(data, dims):\n",
        "            if not dims:\n",
        "                return data.pop(0)\n",
        "            return [reshape_helper(data, dims[1:]) for _ in range(dims[0])]\n",
        "\n",
        "        # Check if the new shape is compatible with the number of elements\n",
        "        if self.prod(yeni_boyut) != len(flat_data):\n",
        "            raise ValueError(\"New shape must have the same number of elements as the original.\")\n",
        "\n",
        "        # Use the helper to create the reshaped data and update the object's internal state\n",
        "        self.__veri = reshape_helper(flat_data, yeni_boyut)\n",
        "        self.__boyut = yeni_boyut\n",
        "    \n",
        "    def relu(self):\n",
        "        # Calculates relu of the elements of the gergen object.\n",
        "        if not (type(self.__veri) == list and type(self.__veri[0]) != list):\n",
        "            raise ValueError(\"Gergen object must be a vector in order to apply relu operation\")\n",
        "        relu_operation = ReLU()\n",
        "        result = relu_operation(self)\n",
        "        return result\n",
        "    \n",
        "    def softmax(self):\n",
        "        # Calculates softmax of a vector gergen.\n",
        "        if not (type(self.__veri) == list and type(self.__veri[0]) != list):\n",
        "            raise ValueError(\"Gergen object must be a vector in order to apply softmax operation\")\n",
        "        softmax_operation = Softmax()\n",
        "        result = softmax_operation(self)\n",
        "        return result\n",
        "    \n",
        "    def vectorAndMatrixProduct(self, W):\n",
        "        # Calculates xW where x is self and W is a matrix\n",
        "        vectorAndMatrixProduct_operation = Mul()\n",
        "        result = vectorAndMatrixProduct_operation(self, W)\n",
        "        return result\n",
        "\n",
        "    def turev_al(self, grad_output=1, direction=\"up\"):\n",
        "        \"\"\"\n",
        "        TODO: Implement the backward pass for the gergen object\n",
        "        \"\"\"\n",
        "        operation = self.operation\n",
        "        if type(operation) != Mul:\n",
        "            self.turev = operation.geri(grad_output)\n",
        "        else:\n",
        "            self.turev = operation.geri(grad_output, direction)\n",
        " \n",
        "            \n",
        "    def update(self, grad_input = 1, learning_rate = 0.1):\n",
        "        if self.requires_grad == True:\n",
        "            for operand in self.operation.operands:\n",
        "                if type(self.operation) != Mul:\n",
        "                    self.turev_al(grad_input)\n",
        "                else:\n",
        "                    if type(operand.get_veri()[0]) != list:\n",
        "                        self.turev_al(grad_input, \"down\")\n",
        "                    else:\n",
        "                        self.turev_al(grad_input, \"up\")\n",
        "                operand.update(self.turev, learning_rate)\n",
        "        else:\n",
        "            if self.requires_update == True:\n",
        "                if len(self.__boyut) == 1:\n",
        "                    n = self.__boyut[0]\n",
        "                    for i in range(n):\n",
        "                        self.__veri[i] -= learning_rate * grad_input.get_veri()[i] \n",
        "                elif len(self.__boyut) == 2:\n",
        "                    n = self.__boyut[0]\n",
        "                    m = self.__boyut[1]\n",
        "                    for i in range(n):\n",
        "                        for j in range(m):\n",
        "                            self.__veri[i][j] -= learning_rate * grad_input.get_veri()[i][j]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ9O1fcCrgeL"
      },
      "source": [
        "# 2 The MLP Implementation\n",
        "\n",
        "Now, you need to complete the MLP implementation. Your task is to complete the MLP implementation by following the steps outlined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pmKGAAcrnZ8"
      },
      "source": [
        "## 2.1 Katman Class\n",
        "\n",
        "To complete MLP implementation, we first need to implement Katman (Layer) class. Implementing the Katman class involves defining its structure and operational methods such as the necessary mathematical operations, integrating activation functions, and setting up mechanisms for learning the layer's parameters during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bi7tAIYrMecf"
      },
      "outputs": [],
      "source": [
        "class Katman:\n",
        "    def __init__(self, input_size, output_size, activation=None):\n",
        "        \"\"\"\n",
        "        TODO: Initialize weights and biases\n",
        "        \"\"\"\n",
        "\n",
        "        # Store input size and output size for faster access.\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Set activation function\n",
        "        self.activation = activation\n",
        "\n",
        "        # He initialization works well with ReLU activation and the MLP that we will implement uses ReLU.\n",
        "        # Therefore, I decided to use He initialization.\n",
        "\n",
        "        mean = 0\n",
        "        standard_deviation = math.sqrt(2 / input_size)\n",
        "        shape_of_weight_matrix = (input_size, output_size)\n",
        "        self.weights = rastgele_gercek(shape_of_weight_matrix, (float(\"-inf\"), float(\"inf\")), \"normal\", mean, standard_deviation)\n",
        "        self.biases = rastgele_gercek((output_size,), (float(\"-inf\"), float(\"inf\")), \"normal\", mean, standard_deviation)\n",
        "        self.weights.requires_update = True\n",
        "        self.biases.requires_update = True\n",
        "        self.weights.requires_grad = False\n",
        "        self.biases.requires_grad = False\n",
        "\n",
        "        # Store net values of each neuron in the layer.\n",
        "        self.nets = None\n",
        "\n",
        "    def ileri(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Implement the forward pass\n",
        "        \"\"\"\n",
        "\n",
        "        if type(x) != gergen:\n",
        "            raise TypeError(\"Input parameter (x) must be a gergen object!\")\n",
        "        \n",
        "        x.requires_update = False\n",
        "\n",
        "        mul_op = Mul()\n",
        "        add_op = Add()\n",
        "        mul_result = mul_op(x, self.weights)\n",
        "        add_result = add_op(mul_result, self.biases)\n",
        "        self.nets = add_result\n",
        "\n",
        "        if self.activation is not None:\n",
        "            if self.activation == \"relu\":\n",
        "                relu_op = ReLU()\n",
        "                self.nets = relu_op(self.nets)\n",
        "            elif self.activation == \"softmax\":\n",
        "                softmax_op = Softmax()\n",
        "                self.nets = softmax_op(self.nets)\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported activation function. Supported activations are 'relu' and 'softmax'.\")\n",
        "\n",
        "        return self.nets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA9RkEM2SbAl"
      },
      "source": [
        "## 2.2 ReLU Operation\n",
        "\n",
        "The `ReLU` class encapsulates the Rectified Linear Unit (ReLU) activation function. Characterized by the formula $f(x) = \\max(0, x)$, the ReLU function modifies the input tensor by setting all its negative elements to zero while preserving the positive values.\n",
        "\n",
        "The implementation of the `ReLU` class needs two principal methods:\n",
        "\n",
        "1. **`ileri(self, x)`:** Termed `ileri` to denote the forward propagation phase, this method applies the ReLU function on an input tensor `x`.\n",
        "\n",
        "2. **`geri(self, grad_input)`:** Labeled `geri`, indicating the backward propagation stage, this function is tasked with calculating the gradient of the ReLU function relative to the input tensor, given a gradient input `grad_input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9hmUxk2snVDl"
      },
      "outputs": [],
      "source": [
        "class ReLU(Operation):\n",
        "    def ileri(self, a):\n",
        "        \"\"\"\n",
        "        TODO: ReLU activation function\n",
        "        \"\"\"\n",
        "        self.operands = [a]\n",
        "        self.outputs = gergen(apply_elementwise(a, lambda x : max(0,x)), operation=self)\n",
        "        self.outputs.requires_grad = True\n",
        "        self.outputs.requires_update = False\n",
        "        return self.outputs\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        TODO: Compute the gradient of the ReLU function\n",
        "        \"\"\"\n",
        "        a = self.operands[0].get_veri()\n",
        "        n = len(a)\n",
        "        gradient = create_zero_square_matrix(n)\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                if i == j:\n",
        "                    if a[i] > 0:\n",
        "                        gradient[i][j] = 1\n",
        "                    else:\n",
        "                        gradient[i][j] = 0\n",
        "                else:\n",
        "                    gradient[i][j] = 0\n",
        "        cumulative_gradient = vector_matrix_product(grad_input.get_veri(), gradient)\n",
        "        return gergen(cumulative_gradient, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bxPgnTtTVct"
      },
      "source": [
        "## 2.3 Softmax Operation\n",
        "\n",
        "The `Softmax` class is designed to implement the Softmax activation function. The Softmax function converts the raw output scores from the model into probabilities by taking the exponential of each output and then normalizing these values by dividing by the sum of all the exponentials. This results in an output vector where each component represents the probability of the corresponding class, and the sum of all components is 1.\n",
        "\n",
        "Implementing the `Softmax` class involves defining two key methods:\n",
        "\n",
        "1. **`ileri(self, x)`:** This method, named `ileri` for the forward pass, applies the Softmax function to an input tensor `x`.\n",
        "\n",
        "2. **`geri(self, grad_input)`:** The `geri` method, indicating the backward pass, is responsible for computing the gradient of the Softmax function with respect to the input tensor, given an input gradient `grad_input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KxyRQS9OvUTo"
      },
      "outputs": [],
      "source": [
        "class Softmax(Operation):\n",
        "    def ileri(self, a):\n",
        "        \"\"\"\n",
        "        Softmax activation function\n",
        "        \"\"\"\n",
        "        self.operands = [a]\n",
        "        max_val = max(a.get_veri())  # Subtract the maximum to prevent \"Result too large\" error !\n",
        "        shifted_exp = [math.exp(entry - max_val) for entry in a.get_veri()]  \n",
        "        exp_sum = sum(shifted_exp)  \n",
        "        self.outputs = gergen(apply_elementwise(a, lambda x : math.exp(x - max_val) / exp_sum), operation=self)\n",
        "        self.outputs.requires_grad = True\n",
        "        self.outputs.requires_update = False\n",
        "        return self.outputs\n",
        "\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        TODO: Compute the gradient of the Softmax function\n",
        "        \"\"\"\n",
        "\n",
        "        a = self.operands[0].get_veri()\n",
        "        n = len(a)\n",
        "        max_val = max(a)  # Subtract the maximum to prevent \"Result too large\" error !\n",
        "        exp_sum = sum([math.e ** (entry - max_val) for entry in a])\n",
        "        epsilon = 1e-15  \n",
        "        if exp_sum == 0:\n",
        "            exp_sum += epsilon\n",
        "        gradient = create_zero_square_matrix(n)\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                if i == j:\n",
        "                    gradient[i][j] = (((math.e ** (a[i] - max_val)) * (exp_sum)) - ((math.e ** (a[i] - max_val)) * (math.e ** (a[i] - max_val)))) / (exp_sum ** 2)\n",
        "                else:\n",
        "                    gradient[i][j] = (-((math.e ** (a[i] - max_val)) * (math.e ** (a[j] - max_val)))) / (exp_sum ** 2)\n",
        "        cumulative_gradient = vector_matrix_product(grad_input.get_veri(), gradient)\n",
        "        return gergen(cumulative_gradient, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFKNHLC9H2WR"
      },
      "source": [
        "## 2.4 MLP Class\n",
        "\n",
        "The `MLP` class is a template for creating our custom MLP.\n",
        "\n",
        "When setting up (`__init__`), you need to define:\n",
        "- `input_size`: The shape of input layer.\n",
        "- `hidden_size`: The shape of the hidden layer.\n",
        "- `output_size`: How many outputs you need at the end, like how many categories you're classifying.\n",
        "\n",
        "The main job of this setup is to prepare the layers with their settings and connections.\n",
        "\n",
        "The `ileri` method takes your data (`x`) and sends it through all the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kDFRKfctMhKu"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        TODO: Initialize the MLP with input, hidden, and output layers\n",
        "        \"\"\"\n",
        "        self.hidden_layer = Katman(input_size, hidden_size, \"relu\")\n",
        "        self.output_layer = Katman(hidden_size, output_size, \"softmax\")\n",
        "\n",
        "        self.h2 = None # h2 = max(0, h1) where h1 = x.W_1 + b1\n",
        "        y_hat = None   # y_hat = softmax(h3) where h3 = h2.W_2 + b2\n",
        "\n",
        "    def ileri(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Implement the forward pass\n",
        "        \"\"\"\n",
        "        self.hidden_layer.ileri(x)\n",
        "        self.h2 = self.hidden_layer.nets\n",
        "        self.output_layer.ileri(self.h2)\n",
        "        self.y_hat = self.output_layer.nets\n",
        "\n",
        "        return self.y_hat\n",
        "    \n",
        "    def geri(self, y_true, learning_rate=0.1):\n",
        "        cross_entropy_op = CrossEntropy()\n",
        "        loss = cross_entropy_op(self.y_hat, y_true)\n",
        "        loss.update(learning_rate)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBF0KFquVFAd"
      },
      "source": [
        "## 2.5 Cross-Entropy Loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sTR6BXBNQ90C"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    TODO: Implement the cross-entropy loss function\n",
        "    y_pred : Predicted probabilities for each class in each sample\n",
        "    y_true : True labels.\n",
        "    Remember, in a multi-class classification context, y_true is typically represented in a one-hot encoded format.\n",
        "    Although it does not matter too much, I have used base e (ln) for the logarithm since it is easier to calculate its derivative.\n",
        "    \"\"\"\n",
        "    \n",
        "    if type(y_pred) != gergen or type(y_true) != gergen:\n",
        "        raise TypeError(\"Prediction and label must be a gergen!\")\n",
        "    \n",
        "    y_pred = y_pred.get_veri()\n",
        "    y_true = y_true.get_veri()\n",
        "\n",
        "    if len(y_pred) != len(y_true):\n",
        "        raise ValueError(\"Shape of prediction and label do not match!\")\n",
        "\n",
        "    n = len(y_pred)\n",
        "    epsilon = 1e-15  \n",
        "\n",
        "    # Avoid log(0)\n",
        "    for i in range(n):\n",
        "        if y_pred[i] == 0:\n",
        "            y_pred[i] += epsilon\n",
        "\n",
        "    sum = 0\n",
        "    for i in range(n):\n",
        "        sum += (y_true[i] * math.log(y_pred[i]))\n",
        "    loss = (-1 / n) * sum\n",
        "    return loss\n",
        "\n",
        "class CrossEntropy(Operation):\n",
        "    def ileri(self, y_pred, y_true): \n",
        "        \"\"\"\n",
        "        Calculates cross entropy loss.\n",
        "        Loss is a scalar valued gergen.\n",
        "        \"\"\"\n",
        "        self.operands = [y_pred, y_true]\n",
        "        self.outputs = gergen(cross_entropy(y_pred, y_true), operation=self)\n",
        "        self.outputs.requires_grad = True\n",
        "        self.outputs.requires_update = False\n",
        "        return self.outputs\n",
        "\n",
        "    def geri(self, grad_input=1): \n",
        "        \"\"\"\n",
        "        Implements the gradient computation for the CrossEntropy operation.\n",
        "        It returns a gergen whose value is a vector describing derivative of loss with respect to y_pred.\n",
        "        \"\"\"\n",
        "        gradients = [] \n",
        "        y_pred = self.operands[0].get_veri()\n",
        "        y_true = self.operands[1].get_veri()\n",
        "        n = len(y_pred)\n",
        "        for i in range(n):\n",
        "            # Gradient of loss with respect to y_i_hat is equal to (-1 / n) * y_i * (1 / y_i_hat)\n",
        "            # In order to avoid division by zero, add an epsilon to y_i_hat if it is zero.\n",
        "            epsilon = 1e-15  \n",
        "            y_i = y_true[i]\n",
        "            y_i_hat = y_pred[i]\n",
        "            if y_i_hat == 0:\n",
        "                y_i_hat += epsilon\n",
        "            gradient = (-1 / n) * y_i * (1 / y_i_hat)\n",
        "            gradients.append(gradient)\n",
        "        # Since Cross Entropy will be at the end of the computational graph, there wont't be grad_input.\n",
        "        # Therefore cumulative gradient will be equal to current gradient.\n",
        "        return gergen(gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zk_5AQtW-Ma"
      },
      "source": [
        "## 2.6 Implementing the training pipeline with `egit()`\n",
        "\n",
        "The `egit()` function adjusts the model's weights and biases to decrease errors and improve predictions through epochs. Here's a simplified overview of its components and steps:\n",
        "\n",
        "### Main Components:\n",
        "\n",
        "- **`mlp`**: The MLP model that we implemented.\n",
        "- **`inputs`**: The data fed into the model.\n",
        "- **`targets`**: The labels for each input\n",
        "- **`epochs`**: The number of complete passes through the training dataset.\n",
        "- **`learning_rate`**: How much the model's weights are adjusted during training to minimize error.\n",
        "\n",
        "You need to implement these training steps:\n",
        "\n",
        "1. **Forward Pass**\n",
        "\n",
        "2. **Calculate Loss**\n",
        "\n",
        "3. **Backward Pass**\n",
        "\n",
        "4. **Update Parameters**\n",
        "\n",
        "5. **Reset Gradients**\n",
        "\n",
        "6. **Loss Reporting**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "svEUrEmMMxLO"
      },
      "outputs": [],
      "source": [
        "def egit(mlp, inputs, targets, epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    TODO: Implement the training loop\n",
        "    \"\"\"\n",
        "\n",
        "    loss_history = []\n",
        "    n = len(inputs)\n",
        "    for epoch in range(epochs):\n",
        "        '''\n",
        "        TODO: Implement training pipeline for each example\n",
        "        '''\n",
        "        epoch_loss_sum = 0\n",
        "        for input, target in zip(inputs, targets):\n",
        "\n",
        "            # Forward pass - with mlp.ileri\n",
        "            input = gergen(input, requires_update=False)\n",
        "            prediction = mlp.ileri(input)\n",
        "\n",
        "            # Calculate Loss - with cross_entropy (I am calculating the loss in mlp.geri. So, in order to print it here, I will recalculate here.)\n",
        "            loss = cross_entropy(prediction, gergen(target))\n",
        "            epoch_loss_sum += loss\n",
        "\n",
        "            # Backward pass - Compute gradients for example\n",
        "            # Update parameters\n",
        "            # Reset gradients\n",
        "            mlp.geri(gergen(target, requires_update=False),learning_rate)\n",
        "\n",
        "        # Print epoch loss here if desired\n",
        "        epoch_average_loss = epoch_loss_sum / n\n",
        "        loss_history.append(epoch_average_loss)\n",
        "        print(f\"Epoch: {epoch}, Loss: {epoch_average_loss}\".format(epoch, loss))\n",
        "\n",
        "\n",
        "    return mlp, loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYj01h9FYRSR"
      },
      "source": [
        "## 2.7 Implementing the testing pipeline with `test()`\n",
        "\n",
        "The `test()` measures the trained model's performance in test data.\n",
        "\n",
        "### Main Components:\n",
        "\n",
        "- **`mlp`**: The model that we trained with egit().\n",
        "- **`inputs`**: Testing data.\n",
        "- **`targets`**: Labels for testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LzW05UXlVP2X"
      },
      "outputs": [],
      "source": [
        "def test(mlp, inputs, targets):\n",
        "    \"\"\"\n",
        "    TODO: Implement the testing pipeline\n",
        "    \"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    for input, target in zip(inputs, targets):\n",
        "        prediction = mlp.ileri(gergen(input))\n",
        "        current_loss = cross_entropy(prediction, gergen(target))\n",
        "        loss += current_loss\n",
        "        \n",
        "    loss = loss / len(inputs)\n",
        "\n",
        "    print(\"Test Loss: {}\".format(loss))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8oQAeqgY8y9"
      },
      "source": [
        "## 2.8 Data Handling Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "he4K9vfbKp4f"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoder(x):\n",
        "    result = []\n",
        "    for i in range(10):\n",
        "        result.append(0)\n",
        "    for i in range(10):\n",
        "        if i == x:\n",
        "            result[i] = 1\n",
        "    return result\n",
        "\n",
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "def data_preprocessing(data_file):\n",
        "    \"\"\"\n",
        "    TODO:    DATA PREPROCESSING\n",
        "    \"\"\"\n",
        "    # Load the data\n",
        "    # Get the first column as labels (You can use one-hot encoding if needed (You can use sklearn or pandas for this))\n",
        "    # Get the remaining columns as data\n",
        "    # Return the data and labels\n",
        "\n",
        "    data = pd.read_csv(data_file, header=None)\n",
        "    y = data.iloc[:, 0]  # First column is label\n",
        "    X = data.iloc[:, 1:]  # Rest of the columns are features\n",
        "    y = list(y.values)\n",
        "    y_one_hot_encoded = []\n",
        "    for label in y:\n",
        "        y_one_hot_encoded.append(one_hot_encoder(label))\n",
        "\n",
        "    list_x = []\n",
        "    for element in list(X.values):\n",
        "        list_x.append(element.tolist())\n",
        "\n",
        "    return list_x, y_one_hot_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-4U07WQZNSE"
      },
      "source": [
        "## 2.9 Training and Testing our custom MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 0.23360122711381573\n",
            "Epoch: 1, Loss: 0.23034202491763822\n",
            "Epoch: 2, Loss: 0.23034202491741443\n",
            "Epoch: 3, Loss: 0.23034202491741443\n",
            "Epoch: 4, Loss: 0.23034202491741443\n",
            "Test Loss: 0.2302213695792011\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "TODO: Implement the training pipeline.\n",
        "'''\n",
        "\n",
        "# Load the data\n",
        "train_data_path = \"./train_data.csv\"\n",
        "test_data_path = \"./test_data.csv\"\n",
        "data, labels = data_preprocessing(train_data_path)\n",
        "test_data, test_labels = data_preprocessing(test_data_path)\n",
        "# Initialize the MLP with input, hidden, and output layers\n",
        "input_size = 28*28\n",
        "hidden_size = 5\n",
        "output_size = 10\n",
        "mlp = MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size)\n",
        "\n",
        "# Train the MLP using your preferred training loop\n",
        "epochs = 5\n",
        "learning_rate = 0.0001\n",
        "\n",
        "trained_mlp, loss_history = egit(mlp, data, labels, epochs, learning_rate)\n",
        "test_loss = test(mlp, test_data, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0_ApnFpR-b"
      },
      "source": [
        "## 2.11 Find the Best Hyperparameters\n",
        "\n",
        "Let us train the model for different values for our hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note: \n",
        "I unnested the for loop and try each hyperparameter combination in a different cell since the for loop takes too much time to finish at a time.\n",
        "\n",
        "In addition to this, I used a subdata which is the first 2000 samples and run 5 epochs to reduce the execution time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_loss = 10\n",
        "best_model = None\n",
        "best_lr = None\n",
        "best_hl = None\n",
        "best_loss_history = None\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.01 and hidden layer_size: 5 ----------\n",
            "\n",
            "Epoch: 0, Loss: 3.1420710392073947\n",
            "Epoch: 1, Loss: 3.1326670190184336\n",
            "Epoch: 2, Loss: 3.1326670190184336\n",
            "Epoch: 3, Loss: 3.1326670190184336\n",
            "Epoch: 4, Loss: 3.1326670190184336\n",
            "Test Loss: 3.1050359979025077\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-2\n",
        "hidden_layer_size = 5\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.001 and hidden layer_size: 5 ----------\n",
            "\n",
            "Epoch: 0, Loss: 0.2504232605584121\n",
            "Epoch: 1, Loss: 0.23034513671895315\n",
            "Epoch: 2, Loss: 0.23031156436867528\n",
            "Epoch: 3, Loss: 0.2303196996101717\n",
            "Epoch: 4, Loss: 0.2303211387313028\n",
            "Test Loss: 0.23031410360350432\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "hidden_layer_size = 5\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.0001 and hidden layer_size: 5 ----------\n",
            "\n",
            "Epoch: 0, Loss: 3.1225436262963866\n",
            "Epoch: 1, Loss: 3.1136706920012327\n",
            "Epoch: 2, Loss: 3.1136706920012327\n",
            "Epoch: 3, Loss: 3.1136706920012327\n",
            "Epoch: 4, Loss: 3.1136706920012327\n",
            "Test Loss: 3.1229961616278614\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-4\n",
        "hidden_layer_size = 5\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 1e-05 and hidden layer_size: 5 ----------\n",
            "\n",
            "Epoch: 0, Loss: 3.1315848426623227\n",
            "Epoch: 1, Loss: 3.1136706920012327\n",
            "Epoch: 2, Loss: 3.1136706920012327\n",
            "Epoch: 3, Loss: 3.1136706920012327\n",
            "Epoch: 4, Loss: 3.1136706920012327\n",
            "Test Loss: 3.1229961616278614\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-5\n",
        "hidden_layer_size = 5\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.01 and hidden layer_size: 10 ----------\n",
            "\n",
            "Epoch: 0, Loss: 0.23740059487192974\n",
            "Epoch: 1, Loss: 0.2304515051370507\n",
            "Epoch: 2, Loss: 0.23032250782971486\n",
            "Epoch: 3, Loss: 0.23032127674483616\n",
            "Epoch: 4, Loss: 0.23032139174571975\n",
            "Test Loss: 0.2303142605529853\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-2\n",
        "hidden_layer_size = 10\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.001 and hidden layer_size: 10 ----------\n",
            "\n",
            "Epoch: 0, Loss: 0.2532521138111286\n",
            "Epoch: 1, Loss: 0.2303779403945083\n",
            "Epoch: 2, Loss: 0.23032192084417336\n",
            "Epoch: 3, Loss: 0.23032139818902972\n",
            "Epoch: 4, Loss: 0.23032139724581455\n",
            "Test Loss: 0.230314214080328\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "hidden_layer_size = 10\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.0001 and hidden layer_size: 10 ----------\n",
            "\n",
            "Epoch: 0, Loss: 3.1214342393716445\n",
            "Epoch: 1, Loss: 3.1136706920012327\n",
            "Epoch: 2, Loss: 3.1136706920012327\n",
            "Epoch: 3, Loss: 3.1136706920012327\n",
            "Epoch: 4, Loss: 3.1136706920012327\n",
            "Test Loss: 3.1229961616278614\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-4\n",
        "hidden_layer_size = 10\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 1e-05 and hidden layer_size: 10 ----------\n",
            "\n",
            "Epoch: 0, Loss: 0.2489593295094669\n",
            "Epoch: 1, Loss: 0.2303774391922424\n",
            "Epoch: 2, Loss: 0.23032105585794854\n",
            "Epoch: 3, Loss: 0.2303212064985641\n",
            "Epoch: 4, Loss: 0.2303213666372526\n",
            "Test Loss: 0.2303141826216031\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-5\n",
        "hidden_layer_size = 10\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.01 and hidden layer_size: 30 ----------\n",
            "\n",
            "Epoch: 0, Loss: 0.2578911112695716\n",
            "Epoch: 1, Loss: 0.23047868294200066\n",
            "Epoch: 2, Loss: 0.23031981321298364\n",
            "Epoch: 3, Loss: 0.2303204331102854\n",
            "Epoch: 4, Loss: 0.23032120925259503\n",
            "Test Loss: 0.23031401140045796\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-2\n",
        "hidden_layer_size = 30\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-bUqdLUt21f"
      },
      "source": [
        "## 2.12 Plot the Loss Curve of the Best Model\n",
        "\n",
        "Let us analyze some aspects of the best model. To keep things short, let us just plot the loss history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.001 and hidden layer_size: 30 ----------\n",
            "\n",
            "Epoch: 0, Loss: 0.24638479497820637\n",
            "Epoch: 1, Loss: 0.230435746233494\n",
            "Epoch: 2, Loss: 0.2303276884340746\n",
            "Epoch: 3, Loss: 0.2303221768714743\n",
            "Epoch: 4, Loss: 0.23032151872163745\n",
            "Test Loss: 0.23031433490839667\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "hidden_layer_size = 30\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 0.0001 and hidden layer_size: 30 ----------\n",
            "\n",
            "Epoch: 0, Loss: 0.24849450990512856\n",
            "Epoch: 1, Loss: 0.23043663501450287\n",
            "Epoch: 2, Loss: 0.23032157302292644\n",
            "Epoch: 3, Loss: 0.23032125299358236\n",
            "Epoch: 4, Loss: 0.23032138099321295\n",
            "Test Loss: 0.2303141726827022\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-4\n",
        "hidden_layer_size = 30\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training for learning rate: 1e-05 and hidden layer_size: 30 ----------\n",
            "\n",
            "Epoch: 0, Loss: 3.120087493364864\n",
            "Epoch: 1, Loss: 3.1136706920012327\n",
            "Epoch: 2, Loss: 3.1136706920012327\n",
            "Epoch: 3, Loss: 3.1136706920012327\n",
            "Epoch: 4, Loss: 3.1136706920012327\n",
            "Test Loss: 3.1229961616278614\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-5\n",
        "hidden_layer_size = 30\n",
        "\n",
        "## @TODO: Create a new MLP instance\n",
        "mlp = MLP(28*28, hidden_size, 10)\n",
        "\n",
        "## @TODO: Create a new loss instance with cross-entropy\n",
        "## @TODO: Train with the training set with current lr and hl for 5 epochs\n",
        "print(f\"\\n---------- Training for learning rate: {learning_rate} and hidden layer_size: {hidden_layer_size} ----------\\n\")\n",
        "trained_mlp, loss_history = egit(mlp, data[:2000], labels[:2000], 5, learning_rate)\n",
        "train_loss = sum(loss_history) / len(loss_history)\n",
        "\n",
        "# @TODO: Predict values for test set and calculate test loss\n",
        "test_loss = test(trained_mlp, test_data, test_labels)\n",
        "\n",
        "# Save the results\n",
        "results[(learning_rate,hidden_layer_size)] = (train_loss, test_loss)\n",
        "if test_loss < best_loss:\n",
        "    best_lr = learning_rate\n",
        "    best_hl = hidden_layer_size\n",
        "    best_loss = test_loss\n",
        "    best_model = mlp\n",
        "    best_loss_history = loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "hRq1YsiruCDL"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "'''\n",
        "TODO: Plot the Loss Curve for the best model using loss_history\n",
        "'''\n",
        "# Generate x values (epochs)\n",
        "epochs = range(1, len(best_loss_history) + 1)\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.plot(epochs, best_loss_history, label='Training Loss')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss history of the best model')\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDGgmS7Zrp48"
      },
      "source": [
        "# 3 Implementation in PyTorch\n",
        "\n",
        "Now, you need to implement the same MLP structure using PyTorch library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6H_WRy2Z_7z"
      },
      "source": [
        "## 3.1 MLP_torch class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMGmeH_kl8xT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: PyTorch implementation\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the MLP architecture\n",
        "class MLP_torch(nn.Module):\n",
        "    \"\"\"\n",
        "    TODO: Implement the MLP architecture\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_size, hidden_layer_size, output_size):\n",
        "        super(MLP_torch, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        # Define the layers\n",
        "        self.input_layer = nn.Linear(input_size, hidden_layer_size)\n",
        "        self.hidden_layer = nn.Linear(hidden_layer_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = torch.relu(self.input_layer(x))\n",
        "        x = torch.softmax(self.hidden_layer(x), dim=0)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffTadGRvaEGr"
      },
      "source": [
        "## 3.2 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlFwrpZqMr5g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # Remove these later\n",
        "\n",
        "'''\n",
        "TODO: Handle the data and labels for PyTorch\n",
        "'''\n",
        "def data_preprocessing_torch(path_to_data, one_hot_encode_labels):\n",
        "    data = pd.read_csv(path_to_data, header=None)\n",
        "    y = data.iloc[:, 0]  # First column is label\n",
        "    X = data.iloc[:, 1:]  # Rest of the columns are features\n",
        "\n",
        "    if one_hot_encode_labels == True:\n",
        "        # One-hot encode the labels\n",
        "        y_onehot = torch.nn.functional.one_hot(torch.tensor(y.values, dtype=torch.long), num_classes=10)\n",
        "\n",
        "        # Convert data to PyTorch tensors\n",
        "        X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "        y_tensor = y_onehot.float()\n",
        "\n",
        "        return X_tensor, y_tensor\n",
        "    \n",
        "    else:\n",
        "        # Convert data to PyTorch tensors\n",
        "        X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y.values, dtype=torch.float32)\n",
        "\n",
        "        return X_tensor, y_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-jc1EsWaG33"
      },
      "source": [
        "## 3.3 Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7-fJsSwmmQm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: Implement the training pipeline using PyTorch\n",
        "'''\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_torch(mlp, inputs, targets, epochs, learning_rate, validation_data, validation_labels):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        # Implement the training pipeline for each example\n",
        "        l = len(inputs)\n",
        "        for i in range(l):\n",
        "            # Forward pass - with mlp.forward\n",
        "            output = mlp.forward(inputs[i])\n",
        "\n",
        "            # Calculate Loss - with criterion (CrossEntropyLoss)\n",
        "            loss = criterion(output.unsqueeze(0), targets[i].unsqueeze(0))\n",
        "\n",
        "            # Backward pass - Compute gradients for example\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Print epoch loss here if desired (I decided to print average loss for that epoch)\n",
        "        print(f'EPOCH: {epoch + 1}, Training Loss: {epoch_loss / len(inputs)}', end=\"\")\n",
        "\n",
        "        training_losses.append(epoch_loss / len(inputs))\n",
        "\n",
        "        validation_loss = test_torch(mlp, validation_data, validation_labels, True)\n",
        "        print(f\", Validation Loss: {validation_loss}\")\n",
        "        validation_losses.append(validation_loss)\n",
        "    \n",
        "    return mlp, training_losses, validation_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBFAMdOxaLwx"
      },
      "source": [
        "## 3.4 Testing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YxTvzVPWx0i"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "TODO: Implement the testing pipeline using PyTorch\n",
        "'''\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def test_torch(mlp, inputs, targets, validating=False):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():  # We don't need gradients for testing\n",
        "        for i in range(len(inputs)):\n",
        "            # Forward pass\n",
        "            output = mlp.forward(inputs[i])\n",
        "\n",
        "            # Calculate Loss\n",
        "            loss = criterion(output.unsqueeze(0), targets[i].unsqueeze(0))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(inputs)\n",
        "    \n",
        "    if validating == False:\n",
        "        print(f'Testing Loss: {average_loss}')\n",
        "        \n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPfHRCo-aOGv"
      },
      "source": [
        "## 3.5 Main code for PyTorch implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5.1 Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfz9GMLhMr5g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 5, learning_rate: 0.01\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.361976157772541, Validation Loss: 2.3551501343250276\n",
            "EPOCH: 2, Training Loss: 2.3617001653552054, Validation Loss: 2.3551501333236695\n",
            "EPOCH: 3, Training Loss: 2.3617001652359964, Validation Loss: 2.35515013256073\n",
            "EPOCH: 4, Training Loss: 2.361700165069103, Validation Loss: 2.355150131559372\n",
            "EPOCH: 5, Training Loss: 2.36170016490221, Validation Loss: 2.35515013051033\n",
            "EPOCH: 6, Training Loss: 2.3617001646876337, Validation Loss: 2.3551501294612884\n",
            "EPOCH: 7, Training Loss: 2.361700164437294, Validation Loss: 2.3551501284599303\n",
            "EPOCH: 8, Training Loss: 2.36170016425848, Validation Loss: 2.35515012717247\n",
            "EPOCH: 9, Training Loss: 2.361700164091587, Validation Loss: 2.355150125837326\n",
            "EPOCH: 10, Training Loss: 2.3617001638054846, Validation Loss: 2.355150124502182\n",
            "EPOCH: 11, Training Loss: 2.3617001635074617, Validation Loss: 2.3551501230239866\n",
            "EPOCH: 12, Training Loss: 2.3617001632452013, Validation Loss: 2.3551501212120054\n",
            "EPOCH: 13, Training Loss: 2.3617001628637313, Validation Loss: 2.3551501195430755\n",
            "EPOCH: 14, Training Loss: 2.3617001625299454, Validation Loss: 2.3551501174926757\n",
            "EPOCH: 15, Training Loss: 2.3617001621723177, Validation Loss: 2.355150115394592\n",
            "EPOCH: 16, Training Loss: 2.361700161707401, Validation Loss: 2.355150112915039\n",
            "EPOCH: 17, Training Loss: 2.3617001610994337, Validation Loss: 2.3551501103401185\n",
            "EPOCH: 18, Training Loss: 2.361700160562992, Validation Loss: 2.3551501072883605\n",
            "EPOCH: 19, Training Loss: 2.361700159943104, Validation Loss: 2.3551501040935516\n",
            "EPOCH: 20, Training Loss: 2.36170015899539, Validation Loss: 2.3551501001358033\n",
            "EPOCH: 21, Training Loss: 2.361700158149004, Validation Loss: 2.355150095796585\n",
            "EPOCH: 22, Training Loss: 2.3617001570403575, Validation Loss: 2.3551500906944276\n",
            "EPOCH: 23, Training Loss: 2.36170015565753, Validation Loss: 2.355150084400177\n",
            "EPOCH: 24, Training Loss: 2.3617001540243625, Validation Loss: 2.355150077486038\n",
            "EPOCH: 25, Training Loss: 2.3617001519680025, Validation Loss: 2.3551500683784483\n",
            "EPOCH: 26, Training Loss: 2.361700149154663, Validation Loss: 2.3551500569820405\n",
            "EPOCH: 27, Training Loss: 2.361700145506859, Validation Loss: 2.3551500415325166\n",
            "EPOCH: 28, Training Loss: 2.3617001402914526, Validation Loss: 2.3551500207901\n",
            "EPOCH: 29, Training Loss: 2.361700132107735, Validation Loss: 2.3551499891757963\n",
            "EPOCH: 30, Training Loss: 2.3617001182436943, Validation Loss: 2.3551499352693557\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 5, learning_rate: 0.001\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.180614365750551, Validation Loss: 2.1629150631427767\n",
            "EPOCH: 2, Training Loss: 2.1603953855991365, Validation Loss: 2.158756337714195\n",
            "EPOCH: 3, Training Loss: 2.1532529608011246, Validation Loss: 2.1560831806898118\n",
            "EPOCH: 4, Training Loss: 2.151760329979658, Validation Loss: 2.1534368713617327\n",
            "EPOCH: 5, Training Loss: 2.148999146056175, Validation Loss: 2.1515575258016586\n",
            "EPOCH: 6, Training Loss: 2.1461611855566503, Validation Loss: 2.1502591326236726\n",
            "EPOCH: 7, Training Loss: 2.141760287219286, Validation Loss: 2.14827891266346\n",
            "EPOCH: 8, Training Loss: 2.1434731107115748, Validation Loss: 2.158956780552864\n",
            "EPOCH: 9, Training Loss: 2.13954296413064, Validation Loss: 2.1439596270799637\n",
            "EPOCH: 10, Training Loss: 2.1397497968792916, Validation Loss: 2.1445394996881486\n",
            "EPOCH: 11, Training Loss: 2.139343769913912, Validation Loss: 2.144995208930969\n",
            "EPOCH: 12, Training Loss: 2.141330165451765, Validation Loss: 2.1439339988708497\n",
            "EPOCH: 13, Training Loss: 2.181465892320871, Validation Loss: 2.162186022901535\n",
            "EPOCH: 14, Training Loss: 2.1667184551715852, Validation Loss: 2.159117710566521\n",
            "EPOCH: 15, Training Loss: 2.1613614300072195, Validation Loss: 2.155919219207764\n",
            "EPOCH: 16, Training Loss: 2.1601476000785826, Validation Loss: 2.151025655055046\n",
            "EPOCH: 17, Training Loss: 2.1604360403597354, Validation Loss: 2.1536566837072373\n",
            "EPOCH: 18, Training Loss: 2.1548578721642495, Validation Loss: 2.148471601510048\n",
            "EPOCH: 19, Training Loss: 2.1532558811903, Validation Loss: 2.1495201211452484\n",
            "EPOCH: 20, Training Loss: 2.1527842242121698, Validation Loss: 2.14758040368557\n",
            "EPOCH: 21, Training Loss: 2.152609128075838, Validation Loss: 2.1460447896957398\n",
            "EPOCH: 22, Training Loss: 2.1509103141129016, Validation Loss: 2.1473752425193786\n",
            "EPOCH: 23, Training Loss: 2.1513743435502053, Validation Loss: 2.1475752373933794\n",
            "EPOCH: 24, Training Loss: 2.152954938840866, Validation Loss: 2.1450816653013227\n",
            "EPOCH: 25, Training Loss: 2.15050841037035, Validation Loss: 2.1465657720804217\n",
            "EPOCH: 26, Training Loss: 2.1493724142074586, Validation Loss: 2.145521969985962\n",
            "EPOCH: 27, Training Loss: 2.148044694703817, Validation Loss: 2.147678458380699\n",
            "EPOCH: 28, Training Loss: 2.149221563088894, Validation Loss: 2.146801303076744\n",
            "EPOCH: 29, Training Loss: 2.14728586191535, Validation Loss: 2.1513259368658066\n",
            "EPOCH: 30, Training Loss: 2.150728280156851, Validation Loss: 2.173152175807953\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 5, learning_rate: 0.0001\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.0206203182697298, Validation Loss: 1.9921249201774598\n",
            "EPOCH: 2, Training Loss: 1.9374092649519443, Validation Loss: 1.9567863961696625\n",
            "EPOCH: 3, Training Loss: 1.9243262484252452, Validation Loss: 1.9391420132637025\n",
            "EPOCH: 4, Training Loss: 1.9231611770391464, Validation Loss: 1.9342132510662078\n",
            "EPOCH: 5, Training Loss: 1.9161227032899857, Validation Loss: 1.9347198386192321\n",
            "EPOCH: 6, Training Loss: 1.910537397223711, Validation Loss: 1.9389556612491607\n",
            "EPOCH: 7, Training Loss: 1.9087841628372668, Validation Loss: 1.942931750679016\n",
            "EPOCH: 8, Training Loss: 1.9059635539531707, Validation Loss: 1.9259217764377594\n",
            "EPOCH: 9, Training Loss: 1.9042226803183555, Validation Loss: 1.9421541024208069\n",
            "EPOCH: 10, Training Loss: 1.9018011915266513, Validation Loss: 1.9177404109716416\n",
            "EPOCH: 11, Training Loss: 1.8991250582098962, Validation Loss: 1.9219388622999192\n",
            "EPOCH: 12, Training Loss: 1.900358434420824, Validation Loss: 1.9283193005800248\n",
            "EPOCH: 13, Training Loss: 1.8993014220952988, Validation Loss: 1.917260458612442\n",
            "EPOCH: 14, Training Loss: 1.8967999984800816, Validation Loss: 1.9175385087966919\n",
            "EPOCH: 15, Training Loss: 1.8965368271529675, Validation Loss: 1.9149618283748626\n",
            "EPOCH: 16, Training Loss: 1.897251931154728, Validation Loss: 1.9244220014333724\n",
            "EPOCH: 17, Training Loss: 1.8937182615458965, Validation Loss: 1.9105138336896896\n",
            "EPOCH: 18, Training Loss: 1.8938009272813796, Validation Loss: 1.9182226986408233\n",
            "EPOCH: 19, Training Loss: 1.8927224448919295, Validation Loss: 1.9117351500034332\n",
            "EPOCH: 20, Training Loss: 1.8958573800444602, Validation Loss: 1.9152477081775665\n",
            "EPOCH: 21, Training Loss: 1.8934832771897316, Validation Loss: 1.9206784182071686\n",
            "EPOCH: 22, Training Loss: 1.8921578722178936, Validation Loss: 1.9156406296014785\n",
            "EPOCH: 23, Training Loss: 1.8907182080745697, Validation Loss: 1.9096241099596023\n",
            "EPOCH: 24, Training Loss: 1.891576017922163, Validation Loss: 1.9133539975881577\n",
            "EPOCH: 25, Training Loss: 1.8895375790774822, Validation Loss: 1.9141052071809768\n",
            "EPOCH: 26, Training Loss: 1.8893384889543057, Validation Loss: 1.915488506436348\n",
            "EPOCH: 27, Training Loss: 1.8884011322200298, Validation Loss: 1.9156374619245529\n",
            "EPOCH: 28, Training Loss: 1.8883379601597785, Validation Loss: 1.9084483611106873\n",
            "EPOCH: 29, Training Loss: 1.8891710936903954, Validation Loss: 1.913396544933319\n",
            "EPOCH: 30, Training Loss: 1.8881663411200047, Validation Loss: 1.9092184380292891\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 5, learning_rate: 1e-05\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.225284993082285, Validation Loss: 2.1704745461225508\n",
            "EPOCH: 2, Training Loss: 2.1329008663654325, Validation Loss: 2.1335725423574448\n",
            "EPOCH: 3, Training Loss: 2.1089087052345277, Validation Loss: 2.1127817932844164\n",
            "EPOCH: 4, Training Loss: 2.096505818068981, Validation Loss: 2.102679386663437\n",
            "EPOCH: 5, Training Loss: 2.086868252122402, Validation Loss: 2.0962147598981855\n",
            "EPOCH: 6, Training Loss: 2.074190980857611, Validation Loss: 2.0862000774621965\n",
            "EPOCH: 7, Training Loss: 2.0603130243003367, Validation Loss: 2.0755558898210524\n",
            "EPOCH: 8, Training Loss: 2.0481738286376, Validation Loss: 2.0670952590703964\n",
            "EPOCH: 9, Training Loss: 2.0393334649860857, Validation Loss: 2.059484173965454\n",
            "EPOCH: 10, Training Loss: 2.030933585160971, Validation Loss: 2.0553319079875947\n",
            "EPOCH: 11, Training Loss: 2.023282589608431, Validation Loss: 2.0489521513223647\n",
            "EPOCH: 12, Training Loss: 2.01732112814188, Validation Loss: 2.0437819808721542\n",
            "EPOCH: 13, Training Loss: 2.011052756905556, Validation Loss: 2.0371054737091066\n",
            "EPOCH: 14, Training Loss: 2.0060750969946386, Validation Loss: 2.0293738498210905\n",
            "EPOCH: 15, Training Loss: 2.0000057585239412, Validation Loss: 2.013050305938721\n",
            "EPOCH: 16, Training Loss: 1.9611157094061376, Validation Loss: 1.9600714245557784\n",
            "EPOCH: 17, Training Loss: 1.9420267565846443, Validation Loss: 1.95293585357666\n",
            "EPOCH: 18, Training Loss: 1.9357180452108382, Validation Loss: 1.9487207912683486\n",
            "EPOCH: 19, Training Loss: 1.9294684853553772, Validation Loss: 1.9467414900064468\n",
            "EPOCH: 20, Training Loss: 1.9251796110153199, Validation Loss: 1.9438676303863525\n",
            "EPOCH: 21, Training Loss: 1.9200627521574498, Validation Loss: 1.9398685557842255\n",
            "EPOCH: 22, Training Loss: 1.9158186941444875, Validation Loss: 1.9400671613454818\n",
            "EPOCH: 23, Training Loss: 1.9128095592141152, Validation Loss: 1.9393946349620819\n",
            "EPOCH: 24, Training Loss: 1.9094891097664832, Validation Loss: 1.9384212428808212\n",
            "EPOCH: 25, Training Loss: 1.9068617619633674, Validation Loss: 1.937565717792511\n",
            "EPOCH: 26, Training Loss: 1.905125197762251, Validation Loss: 1.935180857682228\n",
            "EPOCH: 27, Training Loss: 1.9031532215774059, Validation Loss: 1.9345785833120346\n",
            "EPOCH: 28, Training Loss: 1.9015141219317913, Validation Loss: 1.9330615857362747\n",
            "EPOCH: 29, Training Loss: 1.900073707062006, Validation Loss: 1.931528336405754\n",
            "EPOCH: 30, Training Loss: 1.89911845138669, Validation Loss: 1.9308509047031404\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 10, learning_rate: 0.01\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.373142151987553, Validation Loss: 2.3701446177482604\n",
            "EPOCH: 2, Training Loss: 2.3731001693725586, Validation Loss: 2.370144617843628\n",
            "EPOCH: 3, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446179389953\n",
            "EPOCH: 4, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446181297303\n",
            "EPOCH: 5, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446182250976\n",
            "EPOCH: 6, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446182727812\n",
            "EPOCH: 7, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446184158326\n",
            "EPOCH: 8, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446185112\n",
            "EPOCH: 9, Training Loss: 2.3731001693725586, Validation Loss: 2.370144618654251\n",
            "EPOCH: 10, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446187496185\n",
            "EPOCH: 11, Training Loss: 2.3731001693725586, Validation Loss: 2.370144618844986\n",
            "EPOCH: 12, Training Loss: 2.3731001693725586, Validation Loss: 2.370144618988037\n",
            "EPOCH: 13, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446190834043\n",
            "EPOCH: 14, Training Loss: 2.3731001693725586, Validation Loss: 2.370144619178772\n",
            "EPOCH: 15, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446192741393\n",
            "EPOCH: 16, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446194171907\n",
            "EPOCH: 17, Training Loss: 2.3731001693725586, Validation Loss: 2.370144619512558\n",
            "EPOCH: 18, Training Loss: 2.3731001693725586, Validation Loss: 2.370144619607925\n",
            "EPOCH: 19, Training Loss: 2.3731001693725586, Validation Loss: 2.370144619703293\n",
            "EPOCH: 20, Training Loss: 2.3731001693725586, Validation Loss: 2.370144619846344\n",
            "EPOCH: 21, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446199417116\n",
            "EPOCH: 22, Training Loss: 2.3731001693725586, Validation Loss: 2.370144620037079\n",
            "EPOCH: 23, Training Loss: 2.3731001693725586, Validation Loss: 2.37014462018013\n",
            "EPOCH: 24, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446202754974\n",
            "EPOCH: 25, Training Loss: 2.3731001693725586, Validation Loss: 2.370144620418549\n",
            "EPOCH: 26, Training Loss: 2.3731001693725586, Validation Loss: 2.370144620513916\n",
            "EPOCH: 27, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446205615997\n",
            "EPOCH: 28, Training Loss: 2.3731001693725586, Validation Loss: 2.370144620704651\n",
            "EPOCH: 29, Training Loss: 2.3731001693725586, Validation Loss: 2.370144620847702\n",
            "EPOCH: 30, Training Loss: 2.3731001693725586, Validation Loss: 2.3701446209430697\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 10, learning_rate: 0.001\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.2036257377386095, Validation Loss: 2.3137629805088045\n",
            "EPOCH: 2, Training Loss: 2.058205011981726, Validation Loss: 1.995023910164833\n",
            "EPOCH: 3, Training Loss: 1.9905726257443428, Validation Loss: 1.9845703219413757\n",
            "EPOCH: 4, Training Loss: 2.0021596785604956, Validation Loss: 2.0336219536542894\n",
            "EPOCH: 5, Training Loss: 1.969781297159195, Validation Loss: 1.9136954280614853\n",
            "EPOCH: 6, Training Loss: 1.9017358460187912, Validation Loss: 1.9198915715694427\n",
            "EPOCH: 7, Training Loss: 1.9029653255403043, Validation Loss: 1.9643952652215957\n",
            "EPOCH: 8, Training Loss: 1.8850518706798554, Validation Loss: 1.9196734230518342\n",
            "EPOCH: 9, Training Loss: 1.8726139559030532, Validation Loss: 1.8781210682630538\n",
            "EPOCH: 10, Training Loss: 1.8687191955327989, Validation Loss: 1.8700564084768296\n",
            "EPOCH: 11, Training Loss: 1.8764935498654842, Validation Loss: 1.8921909685373306\n",
            "EPOCH: 12, Training Loss: 1.8622767120063304, Validation Loss: 1.8628619863271714\n",
            "EPOCH: 13, Training Loss: 1.8532116161286831, Validation Loss: 1.8673610275506973\n",
            "EPOCH: 14, Training Loss: 1.8571646792829037, Validation Loss: 1.8632146601200104\n",
            "EPOCH: 15, Training Loss: 1.8555134965360165, Validation Loss: 1.8767075496673584\n",
            "EPOCH: 16, Training Loss: 1.855507491993904, Validation Loss: 1.8688296093463899\n",
            "EPOCH: 17, Training Loss: 1.840360569936037, Validation Loss: 1.8455141822576522\n",
            "EPOCH: 18, Training Loss: 1.8324049017846584, Validation Loss: 1.8396567632436753\n",
            "EPOCH: 19, Training Loss: 1.8295891409933567, Validation Loss: 1.8590900795698166\n",
            "EPOCH: 20, Training Loss: 1.8332375686526299, Validation Loss: 1.8602658794879914\n",
            "EPOCH: 21, Training Loss: 1.8290449158370494, Validation Loss: 1.846452020549774\n",
            "EPOCH: 22, Training Loss: 1.8315810563266277, Validation Loss: 1.8704483533859253\n",
            "EPOCH: 23, Training Loss: 1.8208288574397564, Validation Loss: 1.834924146413803\n",
            "EPOCH: 24, Training Loss: 1.8177279354572295, Validation Loss: 1.8425278263807296\n",
            "EPOCH: 25, Training Loss: 1.824275396889448, Validation Loss: 1.836571032834053\n",
            "EPOCH: 26, Training Loss: 1.8166214809656143, Validation Loss: 1.8329286420345305\n",
            "EPOCH: 27, Training Loss: 1.8175099212110042, Validation Loss: 1.8561564156532289\n",
            "EPOCH: 28, Training Loss: 1.8259054617404937, Validation Loss: 1.8311567467212677\n",
            "EPOCH: 29, Training Loss: 1.8271544011890888, Validation Loss: 1.8229864867210388\n",
            "EPOCH: 30, Training Loss: 1.8118197364449502, Validation Loss: 1.8270570498943328\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 10, learning_rate: 0.0001\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.211033319467306, Validation Loss: 2.117440165734291\n",
            "EPOCH: 2, Training Loss: 2.061615755778551, Validation Loss: 1.9893995086669922\n",
            "EPOCH: 3, Training Loss: 2.001949762713909, Validation Loss: 1.9902308822870254\n",
            "EPOCH: 4, Training Loss: 1.999314552384615, Validation Loss: 1.9857054144859314\n",
            "EPOCH: 5, Training Loss: 1.9961004863739014, Validation Loss: 1.9835601883411407\n",
            "EPOCH: 6, Training Loss: 1.9956337768673897, Validation Loss: 1.9796458137989044\n",
            "EPOCH: 7, Training Loss: 1.9928961892068386, Validation Loss: 1.9816522587776184\n",
            "EPOCH: 8, Training Loss: 1.992034752625227, Validation Loss: 1.983869541811943\n",
            "EPOCH: 9, Training Loss: 1.991138932389021, Validation Loss: 1.9821132047891616\n",
            "EPOCH: 10, Training Loss: 1.9906232792258263, Validation Loss: 1.9844372567653656\n",
            "EPOCH: 11, Training Loss: 1.9878070279598237, Validation Loss: 1.979218206000328\n",
            "EPOCH: 12, Training Loss: 1.9896550591826438, Validation Loss: 1.979859669995308\n",
            "EPOCH: 13, Training Loss: 1.988503469312191, Validation Loss: 1.9765301980495453\n",
            "EPOCH: 14, Training Loss: 1.988167171126604, Validation Loss: 1.9835421788692473\n",
            "EPOCH: 15, Training Loss: 1.9855898216485977, Validation Loss: 1.9797935596227645\n",
            "EPOCH: 16, Training Loss: 1.9866480253517629, Validation Loss: 1.9767934533596039\n",
            "EPOCH: 17, Training Loss: 1.9857075830698014, Validation Loss: 1.9763124478578566\n",
            "EPOCH: 18, Training Loss: 1.985259138840437, Validation Loss: 1.973715853881836\n",
            "EPOCH: 19, Training Loss: 1.9859065188527107, Validation Loss: 1.976486235642433\n",
            "EPOCH: 20, Training Loss: 1.9851731529891492, Validation Loss: 1.983593558883667\n",
            "EPOCH: 21, Training Loss: 1.9839936777055263, Validation Loss: 1.9741886293888091\n",
            "EPOCH: 22, Training Loss: 1.9854088868439197, Validation Loss: 1.9770641263961792\n",
            "EPOCH: 23, Training Loss: 1.9839707625389098, Validation Loss: 1.9773364717483521\n",
            "EPOCH: 24, Training Loss: 1.9835189670264721, Validation Loss: 1.9766123401165008\n",
            "EPOCH: 25, Training Loss: 1.9830057002723216, Validation Loss: 1.9777674164772034\n",
            "EPOCH: 26, Training Loss: 1.9829797476649285, Validation Loss: 1.9743230204105378\n",
            "EPOCH: 27, Training Loss: 1.9822304913818836, Validation Loss: 1.9763552529096604\n",
            "EPOCH: 28, Training Loss: 1.9842486054122448, Validation Loss: 1.9767549046754838\n",
            "EPOCH: 29, Training Loss: 1.982747663283348, Validation Loss: 1.975637111210823\n",
            "EPOCH: 30, Training Loss: 1.983043627256155, Validation Loss: 1.976768662047386\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 10, learning_rate: 1e-05\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.1025040216088295, Validation Loss: 2.000530673146248\n",
            "EPOCH: 2, Training Loss: 1.9103868275284768, Validation Loss: 1.909774671792984\n",
            "EPOCH: 3, Training Loss: 1.8615055870115758, Validation Loss: 1.885444888615608\n",
            "EPOCH: 4, Training Loss: 1.844133985155821, Validation Loss: 1.8722617169857025\n",
            "EPOCH: 5, Training Loss: 1.835138045346737, Validation Loss: 1.8624984098672868\n",
            "EPOCH: 6, Training Loss: 1.8275657064199449, Validation Loss: 1.8555532519817353\n",
            "EPOCH: 7, Training Loss: 1.8218991066038608, Validation Loss: 1.8508471346139908\n",
            "EPOCH: 8, Training Loss: 1.8176263293385506, Validation Loss: 1.847107692217827\n",
            "EPOCH: 9, Training Loss: 1.8116030719935894, Validation Loss: 1.81993636906147\n",
            "EPOCH: 10, Training Loss: 1.7589448896110058, Validation Loss: 1.778939437031746\n",
            "EPOCH: 11, Training Loss: 1.7401815108180045, Validation Loss: 1.7685355078458787\n",
            "EPOCH: 12, Training Loss: 1.731012463438511, Validation Loss: 1.762647729587555\n",
            "EPOCH: 13, Training Loss: 1.7256589573860168, Validation Loss: 1.7594730463266373\n",
            "EPOCH: 14, Training Loss: 1.7222693091511727, Validation Loss: 1.7546429794073104\n",
            "EPOCH: 15, Training Loss: 1.7200061500132084, Validation Loss: 1.7534581067323685\n",
            "EPOCH: 16, Training Loss: 1.7175749706745147, Validation Loss: 1.7526293743371963\n",
            "EPOCH: 17, Training Loss: 1.7158221230447293, Validation Loss: 1.7512066652774811\n",
            "EPOCH: 18, Training Loss: 1.714165405189991, Validation Loss: 1.7499408846616744\n",
            "EPOCH: 19, Training Loss: 1.7127012455701829, Validation Loss: 1.749180392241478\n",
            "EPOCH: 20, Training Loss: 1.711877189475298, Validation Loss: 1.7486366760253906\n",
            "EPOCH: 21, Training Loss: 1.7106884980916977, Validation Loss: 1.7477746521949769\n",
            "EPOCH: 22, Training Loss: 1.7095099155664444, Validation Loss: 1.7468189496517181\n",
            "EPOCH: 23, Training Loss: 1.7085311016261577, Validation Loss: 1.7459909459352494\n",
            "EPOCH: 24, Training Loss: 1.7075937930285932, Validation Loss: 1.7453783557891847\n",
            "EPOCH: 25, Training Loss: 1.706589422363043, Validation Loss: 1.74471974670887\n",
            "EPOCH: 26, Training Loss: 1.7059173712313176, Validation Loss: 1.7440322676897049\n",
            "EPOCH: 27, Training Loss: 1.7052969871640205, Validation Loss: 1.7441688866376877\n",
            "EPOCH: 28, Training Loss: 1.7048732287406922, Validation Loss: 1.7441669709444045\n",
            "EPOCH: 29, Training Loss: 1.7044369927048684, Validation Loss: 1.7436281383514405\n",
            "EPOCH: 30, Training Loss: 1.7037341254651546, Validation Loss: 1.7433361905813216\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 30, learning_rate: 0.01\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.359250221347809, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 2, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 3, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 4, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 5, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 6, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 7, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 8, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 9, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 10, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 11, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 12, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 13, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 14, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 15, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 16, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 17, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 18, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 19, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 20, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 21, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 22, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 23, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 24, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 25, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 26, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 27, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 28, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 29, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "EPOCH: 30, Training Loss: 2.3592001693725586, Validation Loss: 2.3687501693725586\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 30, learning_rate: 0.001\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.1923618061721326, Validation Loss: 2.1058287094831467\n",
            "EPOCH: 2, Training Loss: 2.096490156465769, Validation Loss: 2.0511263855695723\n",
            "EPOCH: 3, Training Loss: 1.9810743894457816, Validation Loss: 1.9808707370996474\n",
            "EPOCH: 4, Training Loss: 1.9516651836156844, Validation Loss: 1.9754932389259339\n",
            "EPOCH: 5, Training Loss: 1.9497070500433444, Validation Loss: 1.9537958791971206\n",
            "EPOCH: 6, Training Loss: 1.9563042614221573, Validation Loss: 1.9615165838241577\n",
            "EPOCH: 7, Training Loss: 1.9363342218995094, Validation Loss: 1.9579808551311493\n",
            "EPOCH: 8, Training Loss: 1.938357068437338, Validation Loss: 1.9372387347459794\n",
            "EPOCH: 9, Training Loss: 1.926964868658781, Validation Loss: 1.9185656942605973\n",
            "EPOCH: 10, Training Loss: 1.9194866091489793, Validation Loss: 1.9285095175981521\n",
            "EPOCH: 11, Training Loss: 1.9230671054720878, Validation Loss: 1.9602355320453644\n",
            "EPOCH: 12, Training Loss: 1.8912488673388959, Validation Loss: 1.9227721505403519\n",
            "EPOCH: 13, Training Loss: 1.876113901913166, Validation Loss: 1.8664232596874237\n",
            "EPOCH: 14, Training Loss: 1.8649809956610204, Validation Loss: 1.8672562977313996\n",
            "EPOCH: 15, Training Loss: 1.8402659839212894, Validation Loss: 1.8328157106876373\n",
            "EPOCH: 16, Training Loss: 1.8410862286388874, Validation Loss: 1.9095712054729461\n",
            "EPOCH: 17, Training Loss: 1.865911070418358, Validation Loss: 1.814004946422577\n",
            "EPOCH: 18, Training Loss: 1.7889195685505868, Validation Loss: 1.846855579161644\n",
            "EPOCH: 19, Training Loss: 1.8035696697175503, Validation Loss: 1.8186303636312484\n",
            "EPOCH: 20, Training Loss: 1.7808650212705135, Validation Loss: 1.8046452363729477\n",
            "EPOCH: 21, Training Loss: 1.78828852712512, Validation Loss: 1.7574584472894668\n",
            "EPOCH: 22, Training Loss: 1.754967508125305, Validation Loss: 1.7730297417640686\n",
            "EPOCH: 23, Training Loss: 1.7264221936762334, Validation Loss: 1.8141037338972092\n",
            "EPOCH: 24, Training Loss: 1.7235905593693257, Validation Loss: 1.7415422820329667\n",
            "EPOCH: 25, Training Loss: 1.7375060909330844, Validation Loss: 1.7535252564191819\n",
            "EPOCH: 26, Training Loss: 1.721736454987526, Validation Loss: 1.7181514847040176\n",
            "EPOCH: 27, Training Loss: 1.7242361232995986, Validation Loss: 1.7746223669290542\n",
            "EPOCH: 28, Training Loss: 1.7571706089794636, Validation Loss: 1.795357630944252\n",
            "EPOCH: 29, Training Loss: 1.769793159878254, Validation Loss: 1.8091924226284026\n",
            "EPOCH: 30, Training Loss: 1.7797408468782903, Validation Loss: 1.817472705435753\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 30, learning_rate: 0.0001\n",
            "\n",
            "EPOCH: 1, Training Loss: 1.9315089086532593, Validation Loss: 1.7781877963781356\n",
            "EPOCH: 2, Training Loss: 1.6813608280539514, Validation Loss: 1.679385595703125\n",
            "EPOCH: 3, Training Loss: 1.6482859372138976, Validation Loss: 1.6758245970010757\n",
            "EPOCH: 4, Training Loss: 1.6360316088557243, Validation Loss: 1.6540503816366197\n",
            "EPOCH: 5, Training Loss: 1.628901953625679, Validation Loss: 1.665816807794571\n",
            "EPOCH: 6, Training Loss: 1.6206045052111149, Validation Loss: 1.644572562646866\n",
            "EPOCH: 7, Training Loss: 1.6177777063369752, Validation Loss: 1.6447799455881118\n",
            "EPOCH: 8, Training Loss: 1.6125376198232173, Validation Loss: 1.639571176457405\n",
            "EPOCH: 9, Training Loss: 1.6123636288166046, Validation Loss: 1.646255828499794\n",
            "EPOCH: 10, Training Loss: 1.607061276614666, Validation Loss: 1.6347474007844924\n",
            "EPOCH: 11, Training Loss: 1.6078273425996303, Validation Loss: 1.6361207861423492\n",
            "EPOCH: 12, Training Loss: 1.605448060786724, Validation Loss: 1.6333717461586\n",
            "EPOCH: 13, Training Loss: 1.6030773234903812, Validation Loss: 1.6281963254451752\n",
            "EPOCH: 14, Training Loss: 1.6002550486922265, Validation Loss: 1.608662626695633\n",
            "EPOCH: 15, Training Loss: 1.5489075107693673, Validation Loss: 1.5622723526239395\n",
            "EPOCH: 16, Training Loss: 1.5312666902542114, Validation Loss: 1.5548385015964508\n",
            "EPOCH: 17, Training Loss: 1.5245044386208058, Validation Loss: 1.551907966852188\n",
            "EPOCH: 18, Training Loss: 1.5208877005517483, Validation Loss: 1.55297051050663\n",
            "EPOCH: 19, Training Loss: 1.5189440052866936, Validation Loss: 1.5424968097686769\n",
            "EPOCH: 20, Training Loss: 1.5155728874266148, Validation Loss: 1.545082212281227\n",
            "EPOCH: 21, Training Loss: 1.5133911676704883, Validation Loss: 1.5466751687049867\n",
            "EPOCH: 22, Training Loss: 1.5132321003317832, Validation Loss: 1.5458094740629196\n",
            "EPOCH: 23, Training Loss: 1.512259560084343, Validation Loss: 1.5456871420145035\n",
            "EPOCH: 24, Training Loss: 1.510568319672346, Validation Loss: 1.543365556025505\n",
            "EPOCH: 25, Training Loss: 1.5100777007520199, Validation Loss: 1.541501118516922\n",
            "EPOCH: 26, Training Loss: 1.5083131277918815, Validation Loss: 1.541478118467331\n",
            "EPOCH: 27, Training Loss: 1.505191966086626, Validation Loss: 1.5367571326494216\n",
            "EPOCH: 28, Training Loss: 1.5045687099874019, Validation Loss: 1.537013750052452\n",
            "EPOCH: 29, Training Loss: 1.5046449087440967, Validation Loss: 1.5362331190347671\n",
            "EPOCH: 30, Training Loss: 1.5032796864748001, Validation Loss: 1.542111497116089\n",
            "\n",
            "---------- Training ----------\n",
            "\n",
            "Hyperparameters: hidden_size = 30, learning_rate: 1e-05\n",
            "\n",
            "EPOCH: 1, Training Loss: 2.063975504004955, Validation Loss: 2.0132209017038347\n",
            "EPOCH: 2, Training Loss: 1.9518896404206754, Validation Loss: 1.9404797772169113\n",
            "EPOCH: 3, Training Loss: 1.9062283860683442, Validation Loss: 1.9141461203575134\n",
            "EPOCH: 4, Training Loss: 1.8938530319750309, Validation Loss: 1.9068985657691955\n",
            "EPOCH: 5, Training Loss: 1.8884468103706837, Validation Loss: 1.901770641899109\n",
            "EPOCH: 6, Training Loss: 1.8843127338528634, Validation Loss: 1.8999193418502807\n",
            "EPOCH: 7, Training Loss: 1.881383698028326, Validation Loss: 1.8984451767206192\n",
            "EPOCH: 8, Training Loss: 1.879579718989134, Validation Loss: 1.8969772763252257\n",
            "EPOCH: 9, Training Loss: 1.8782519348442555, Validation Loss: 1.8949028049707413\n",
            "EPOCH: 10, Training Loss: 1.8737994632184505, Validation Loss: 1.8709406280994416\n",
            "EPOCH: 11, Training Loss: 1.8508306388616562, Validation Loss: 1.8512697491407395\n",
            "EPOCH: 12, Training Loss: 1.8335632395744323, Validation Loss: 1.8402387136220932\n",
            "EPOCH: 13, Training Loss: 1.824161362105608, Validation Loss: 1.8343393676757813\n",
            "EPOCH: 14, Training Loss: 1.818503677624464, Validation Loss: 1.8304120145082474\n",
            "EPOCH: 15, Training Loss: 1.8146842771470546, Validation Loss: 1.828956442141533\n",
            "EPOCH: 16, Training Loss: 1.8112645571172237, Validation Loss: 1.8275706440210342\n",
            "EPOCH: 17, Training Loss: 1.8084257470607759, Validation Loss: 1.8260338260173798\n",
            "EPOCH: 18, Training Loss: 1.805773530960083, Validation Loss: 1.8251027540683746\n",
            "EPOCH: 19, Training Loss: 1.80396360989213, Validation Loss: 1.8239973338842392\n",
            "EPOCH: 20, Training Loss: 1.8020946735799312, Validation Loss: 1.8232628046274184\n",
            "EPOCH: 21, Training Loss: 1.8007335995256901, Validation Loss: 1.822011350274086\n",
            "EPOCH: 22, Training Loss: 1.7995829568982125, Validation Loss: 1.8206045541763305\n",
            "EPOCH: 23, Training Loss: 1.7985783562481403, Validation Loss: 1.8200078886270523\n",
            "EPOCH: 24, Training Loss: 1.7976587852656842, Validation Loss: 1.8192564007043839\n",
            "EPOCH: 25, Training Loss: 1.7967416558682918, Validation Loss: 1.818514858531952\n",
            "EPOCH: 26, Training Loss: 1.7959047124445437, Validation Loss: 1.8177342579364777\n",
            "EPOCH: 27, Training Loss: 1.7951464257597922, Validation Loss: 1.8171362384080887\n",
            "EPOCH: 28, Training Loss: 1.794555832773447, Validation Loss: 1.8166705644369125\n",
            "EPOCH: 29, Training Loss: 1.7940710757434368, Validation Loss: 1.8164867411136627\n",
            "EPOCH: 30, Training Loss: 1.793635761386156, Validation Loss: 1.816320305109024\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "TODO: Train and test the PyTorch model\n",
        "'''\n",
        "\n",
        "# Load the data\n",
        "train_data, train_labels = data_preprocessing_torch(\"./train_data.csv\", True)\n",
        "test_data, test_labels = data_preprocessing_torch(\"./test_data.csv\", True)\n",
        "validation_data, validation_labels = test_data[:5000], test_labels[:5000] # Split the test set into test and validation set with ratio (3:1)\n",
        "test_data, test_labels = test_data[5000:], test_labels[:5000:]\n",
        "\n",
        "# Initialize the MLP with input, hidden, and output layers\n",
        "input_size = 784\n",
        "output_size = 10\n",
        "\n",
        "epochs = 30 # I have seen that some parameter combinations does not converge in 10 epochs so I decided to train 30 epochs.\n",
        "\n",
        "# Hyperparameter grid\n",
        "hidden_sizes = [5, 10, 30]\n",
        "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
        "\n",
        "# Store grid-seach results\n",
        "results = []\n",
        "for hidden_size in hidden_sizes:\n",
        "    for learning_rate in learning_rates:\n",
        "        print(\"\\n---------- Training ----------\\n\")\n",
        "        print(f\"Hyperparameters: hidden_size = {hidden_size}, learning_rate: {learning_rate}\\n\")\n",
        "        mlp = MLP_torch(input_size=input_size, hidden_layer_size=hidden_size, output_size=output_size)\n",
        "        trained_torch_mlp, training_losses, validation_losses = train_torch(mlp, train_data, train_labels, epochs, learning_rate, validation_data, validation_labels)\n",
        "        results.append((hidden_size, learning_rate, training_losses, validation_losses, trained_torch_mlp))\n",
        "\n",
        "\n",
        "# Testing will be done in the next cell !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5.2 Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------- Testing ----------\n",
            "\n",
            "Testing Loss: 2.3581408536672592\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2.3581408536672592"
            ]
          },
          "execution_count": 2387,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We obtained the lowest validation loss when hidden_size = 30 and learning_rate = 0.0001\n",
        "best_result = None\n",
        "for result in results:\n",
        "    if result[0] == 30 and result[1] == 0.0001:\n",
        "        best_result = result\n",
        "\n",
        "print(\"\\n---------- Testing ----------\\n\")\n",
        "test_torch(best_result[4],test_data, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5.3 Visualize losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_losses = best_result[2]\n",
        "validation_losses = best_result[3]\n",
        "\n",
        "# Generate x values (epochs)\n",
        "epochs = range(1, len(training_losses) + 1)\n",
        "\n",
        "# Plot training and validation losses\n",
        "plt.plot(epochs, training_losses, label='Training Loss')\n",
        "plt.plot(epochs, validation_losses, label='Validation Loss')\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Zaw1gxIGOVT9",
        "ij6-IOwKOYEw",
        "coWq1h40OhFz",
        "PeIxz7MXOjUo",
        "XDp86vtgOnH8",
        "SZIOTmN-OxHS"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
